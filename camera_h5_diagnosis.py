import numpy as np
import cv2
from PIL import ImageFont, ImageDraw, Image
import time
import os
import ollama
import onnxruntime as ort

# --- ì„¤ì • ---
CAPTURE_INTERVAL = 1  # ìº¡ì²˜ ê°„ê²© (ì´ˆ)
CAPTURE_COUNT = 5     # ìº¡ì²˜ íšŸìˆ˜
CAPTURE_FOLDER = "captures" # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥ í´ë”
OLLAMA_MODEL = "gemma3:1b" # ì‚¬ìš©í•  Ollama ëª¨ë¸

DISPLAY_UPDATE_INTERVAL_MS = 400 # í™”ë©´ì— í‘œì‹œë˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë°€ë¦¬ì´ˆ)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
ONNX_MODEL_PATH = "./model/skin_model.onnx"
TFLITE_MODEL_PATH = "./model/skin_model_quantized.tflite"

# --- í´ë˜ìŠ¤ ë° ëª¨ë¸ ì„¤ì • ---
# í´ë˜ìŠ¤ëª… (7ê°œ í´ë˜ìŠ¤)
class_names_kr = [
    'ê¸°ì €ì„¸í¬ì•”',
    'í‘œí”¼ë‚­ì¢…',
    'í˜ˆê´€ì¢…',
    'ë¹„ë¦½ì¢…',
    'ì •ìƒí”¼ë¶€',
    'í¸í‰ì„¸í¬ì•”',
    'ì‚¬ë§ˆê·€'
]

# --- ONNX ëª¨ë¸ í´ë˜ìŠ¤ ---
class ONNXModel:
    def __init__(self, model_path):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        print(f"ONNX ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """ONNX ëª¨ë¸ ì˜ˆì¸¡"""
        result = self.session.run([self.output_name], {self.input_name: input_data})
        return result[0]

# --- TFLite ëª¨ë¸ í´ë˜ìŠ¤ ---
class TFLiteModel:
    def __init__(self, model_path):
        """TFLite ëª¨ë¸ ë¡œë“œ"""
        import tensorflow as tf
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        # ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        print(f"TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
    
    def predict(self, input_data):
        """TFLite ëª¨ë¸ ì˜ˆì¸¡"""
        # ì…ë ¥ ë°ì´í„° ì„¤ì •
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        
        # ì¶”ë¡  ì‹¤í–‰
        self.interpreter.invoke()
        
        # ì¶œë ¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output_data


def get_solution_from_gemma(disease_name):
    """
    ë¡œì»¬ Ollamaì˜ Gemma3 ëª¨ë¸ì—ê²Œ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ê°€ì´ë“œ ìš”ì²­.
    ì‘ë‹µì€ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 5ë‹¨ê³„ë¡œ ìš”ì•½ë˜ë©°, 200ì ë‚´ì™¸ë¡œ ì œí•œë¨.
    """

    prompt = f"""
ë‹¹ì‹ ì€ í”¼ë¶€ ê±´ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ í”¼ë¶€ ì§ˆí™˜ì— ëŒ€í•´ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

í”¼ë¶€ ì§ˆí™˜ëª…: {disease_name}

ì•„ë˜ í˜•ì‹ì— ë”°ë¼ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”:

1. ì§ˆí™˜ ì„¤ëª…: ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨íˆ
2. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­: ì‘ê¸‰ì„± ì—¬ë¶€ í¬í•¨
3. ê°€ì • ê´€ë¦¬ ë°©ë²•: ì†ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥í•œ íŒ
4. ì „ë¬¸ ì¹˜ë£Œ ë°©ë²•: ë³‘ì›ì—ì„œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹˜ë£Œ
5. ì£¼ì˜ì‚¬í•­: ì¬ë°œ, ê°ì—¼, ìê°€ ì¹˜ë£Œ ê²½ê³  ë“±


ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
ë‹µë³€ì€ 200ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """.strip()

    print(f"\n[{OLLAMA_MODEL} ëª¨ë¸ì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.]")

    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response['message']['content'].strip()

    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\nOllama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."


# --- ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_model():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    model = None
    model_type = None
    
    # 1. ONNX ëª¨ë¸ ì‹œë„
    if os.path.exists(ONNX_MODEL_PATH):
        try:
            model = ONNXModel(ONNX_MODEL_PATH)
            model_type = "ONNX"
            print("ONNX ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. TFLite ëª¨ë¸ ì‹œë„ (ONNX ì‹¤íŒ¨ ì‹œ)
    if model is None and os.path.exists(TFLITE_MODEL_PATH):
        try:
            model = TFLiteModel(TFLITE_MODEL_PATH)
            model_type = "TFLite"
            print("TFLite ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ì›ë³¸ H5 ëª¨ë¸ ì‹œë„ (ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ)
    if model is None:
        try:
            import tensorflow as tf
            from tensorflow import keras
            h5_model_path = "C:/Users/kccistc/project/onnx_skin_diagnosis/model/skin_model.h5"
            model = keras.models.load_model(h5_model_path)
            model_type = "H5"
            print("ì›ë³¸ H5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"H5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return model, model_type
# --- TTS ---
from gtts import gTTS
import os

def speak_korean_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        original = "tts_output_original.mp3"
        faster = "tts_output_fast.mp3"

        tts.save(original)

        # ğŸ› ï¸ ffmpegë¡œ ì†ë„ 1.5ë°° ë¹ ë¥´ê²Œ ë³€í™˜ (tempo=1.5)
        os.system(f"ffmpeg -y -i {original} -filter:a 'atempo=1.5' {faster}")

        # ğŸš ì¬ìƒ
        os.system(f"mpg123 {faster}")

        # ğŸ§¹ ì •ë¦¬
        os.remove(original)
        os.remove(faster)
        print(f"[ğŸ§¹] mp3 íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ..")

    except Exception as e:
        print(f"[TTS ì˜¤ë¥˜] {e}")


# --- ë©”ì¸ ë¡œì§ ---
def main():
    print("ONNX Skin Diagnosis System")
    print("=" * 50)
    
    # ìº¡ì²˜ í´ë” ìƒì„±
    if not os.path.exists(CAPTURE_FOLDER):
        os.makedirs(CAPTURE_FOLDER)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model, model_type = initialize_model()
    if model is None:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € convert_h5_to_onnx.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ì„¸ìš”.")
        return
    
    print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_type}")
    
    # í°íŠ¸ ì„¤ì •
    font_path = "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"
    try:
        font = ImageFont.truetype(font_path, 20)
    except IOError:
        print(f"ì˜¤ë¥˜: í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()

    # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë³€ìˆ˜
    last_display_update_time = time.time()
    current_display_label = ""

    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture(1) # ì™¸ë¶€ ì›¹ìº 
    if not cap.isOpened():
        cap = cv2.VideoCapture(0) # ë‚´ì¥ ì›¹ìº 
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    print("ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í™”ë©´ì„ ë³´ë©° ì§„ë‹¨í•  ë¶€ìœ„ë¥¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”.")
    print("í‚¤ë³´ë“œ 'c'ë¥¼ ëˆ„ë¥´ë©´ 5ì´ˆê°„ ì—°ì†ìœ¼ë¡œ ì´¬ì˜í•˜ì—¬ ì§„ë‹¨í•©ë‹ˆë‹¤.")
    print("í‚¤ë³´ë“œ 'q'ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜¤ë¥˜: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ì¤‘ì•™ 1:1 ì˜ì—­ crop
        h, w, _ = frame.shape
        min_dim = min(h, w)
        start_x = (w - min_dim) // 2
        start_y = (h - min_dim) // 2
        crop_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]

        # --- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ---
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_array = cv2.resize(crop_frame, (96, 96))
        img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_array = img_array.astype(np.float32) / 255.0  # ì •ê·œí™”

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
        if model_type == "H5":
            predictions = model.predict(img_array, verbose=0)
        else:
            predictions = model.predict(img_array)
        
        predicted_class_idx = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class_idx]

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        current_label = f"{class_names_kr[predicted_class_idx]} ({confidence*100:.1f}%)"
        
        # í™”ë©´ í‘œì‹œ ì—…ë°ì´íŠ¸ ì£¼ê¸° ì œì–´
        current_time = time.time()
        if (current_time - last_display_update_time) * 1000 >= DISPLAY_UPDATE_INTERVAL_MS:
            current_display_label = current_label
            last_display_update_time = current_time

        # í™”ë©´ì— í‘œì‹œ (Pillow ì‚¬ìš©)
        img_pil = Image.fromarray(cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        draw.text((10, 10), f"ì‹¤ì‹œê°„ ì˜ˆì¸¡ ({model_type}):", font=font, fill=(0, 255, 0))
        draw.text((10, 35), current_display_label, font=font, fill=(0, 255, 0))
        display_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

        cv2.imshow('ONNX Skin Disease Diagnosis', display_frame)

        key = cv2.waitKey(1) & 0xFF

        # --- 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ì—°ì† ìº¡ì²˜ ë° ì§„ë‹¨ ---
        if key == ord('c'):
            # í™”ë©´ì„ ê²€ê²Œ ë§Œë“¤ê³  "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..." ë©”ì‹œì§€ í‘œì‹œ
            black_screen = np.zeros_like(display_frame)
            
            # Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
            img_pil_black = Image.fromarray(cv2.cvtColor(black_screen, cv2.COLOR_BGR2RGB))
            draw_black = ImageDraw.Draw(img_pil_black)
            
            text = "ì˜ì‚¬ì˜ ë‹µë³€ ì¤€ë¹„ì¤‘..."
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            try:
                # Pillow 10.0.0 ì´ìƒ
                text_bbox = draw_black.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except AttributeError:
                # ì´ì „ ë²„ì „ì˜ Pillow
                text_width, text_height = draw_black.textsize(text, font=font)

            text_x = (black_screen.shape[1] - text_width) // 2
            text_y = (black_screen.shape[0] - text_height) // 2
            
            draw_black.text((text_x, text_y), text, font=font, fill=(255, 255, 255))
            
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            black_screen_with_text = cv2.cvtColor(np.array(img_pil_black), cv2.COLOR_RGB2BGR)
            cv2.imshow('ONNX Skin Disease Diagnosis', black_screen_with_text)
            cv2.waitKey(1) # í™”ë©´ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸

            print("\n" + "="*40)
            print(f"ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. {CAPTURE_COUNT}ì´ˆ ë™ì•ˆ {CAPTURE_COUNT}ë²ˆ ì´¬ì˜í•©ë‹ˆë‹¤.")
            print("="*40)
            
            captured_classes = []
            
            for i in range(CAPTURE_COUNT):
                time.sleep(CAPTURE_INTERVAL)
                
                # í˜„ì¬ í”„ë ˆì„(crop_frame)ìœ¼ë¡œ ì˜ˆì¸¡
                current_img_array = cv2.resize(crop_frame, (96, 96))
                current_img_array = np.expand_dims(current_img_array, axis=0)
                current_img_array = current_img_array.astype(np.float32) / 255.0

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì˜ˆì¸¡
                if model_type == "H5":
                    current_predictions = model.predict(current_img_array, verbose=0)
                else:
                    current_predictions = model.predict(current_img_array)
                
                current_predicted_idx = np.argmax(current_predictions[0])
                
                predicted_name = class_names_kr[current_predicted_idx]
                captured_classes.append(predicted_name)
                
                # ìº¡ì²˜ ì´ë¯¸ì§€ ì €ì¥
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                capture_path = os.path.join(CAPTURE_FOLDER, f"capture_{timestamp}_{i+1}.png")
                cv2.imwrite(capture_path, crop_frame)
                
                print(f"ì´¬ì˜ {i+1}/5... ì˜ˆì¸¡: {predicted_name} (ì´ë¯¸ì§€ ì €ì¥: {capture_path})")

            # --- ìµœì¢… ì§„ë‹¨ ---
            print("\n" + "-"*40)
            if len(set(captured_classes)) == 1:
                final_diagnosis = captured_classes[0]
                print(f"ìµœì¢… ì§„ë‹¨ ê²°ê³¼: **{final_diagnosis}**")
                print(f"ì‚¬ìš© ëª¨ë¸: {model_type}")
                print("-"*40)
                
                # Gemma3 í•´ê²°ì±… ìš”ì²­
                solution = get_solution_from_gemma(final_diagnosis)
                print("\n[Ollama Gemma3ì˜ ê±´ê°• ì¡°ì–¸]")
                print(solution)
                print("\n(ì£¼ì˜: ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œê¸°ê´€ì„ ë°©ë¬¸í•˜ì„¸ìš”.)")
                speak_korean_gtts(solution) # TTS ìŒì„± ì¶œë ¥
                
            else:
                print("ì§„ë‹¨ ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ë‚œ {CAPTURE_COUNT}ë²ˆì˜ ì˜ˆì¸¡: {captured_classes}")
            
            print("="*40)
            print("\në‹¤ì‹œ ì§„ë‹¨í•˜ë ¤ë©´ 'c'ë¥¼, ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

        # --- 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ ---
        elif key == ord('q'):
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
